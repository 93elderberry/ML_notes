#!/usr/bin/env python
# coding: utf-8

# # Часть 1. Исследовательский анализ данных с помощью Pandas 
# 
# конспект 1 части курса 'Open Machine Learning Course' mlcourse.ai
#  

# ## 1. Демонстрация основных методов Pandas 

# Мы продемонстрируем основные методы в действии, проанализировав набор данных об оттоке клиентов операторов связи. Давайте прочитаем данные (используя read_csv метод) и посмотрим на первые 5 строк, используя head метод:

# In[1]:


import numpy as np
import pandas as pd

pd.set_option("display.precision", 2)


# In[2]:


DATA_URL = "https://raw.githubusercontent.com/Yorko/mlcourse.ai/main/data/"


# In[3]:


df = pd.read_csv(DATA_URL + "telecom_churn.csv")
df.head()


# In[4]:


#Печать столбцов
print(df.columns)


# In[5]:


#общая информация
print(df.info())


# In[6]:


#изменить тип столбца
df["Churn"] = df["Churn"].astype("int64")


# In[7]:


#Метод describeпоказывает основные статистические характеристики каждого числового признака 
#( int64и float64типов): количество непропущенных значений, среднее значение, стандартное отклонение, диапазон, 
#медиана, 0,25 и 0,75 квартили.
df.describe()


# In[8]:


# Чтобы увидеть статистику по нечисловым признакам, нужно явно указать интересующие типы данных в include параметре.
df.describe(include=["object", "bool"])


# In[9]:


#Для категориальных (тип object) и логических (тип bool) функций мы можем использовать этот value_counts метод.
df["Churn"].value_counts()
#2850 пользователей из 3333 лояльны ; их Churn значение равно 0


# In[10]:


#Для вычисления долей используйте normalize=True к value_counts функции.
df["Churn"].value_counts(normalize=True)


# ###  Сортировка(sort_values)

# DataFrame можно отсортировать по значению одной из переменных (т.е. столбцов). Например, мы можем отсортировать по общей стоимости дня (используйте ascending=False для сортировки в порядке убывания):

# In[11]:


df.sort_values(by="Total day charge", ascending=False).head()


# In[12]:


#Мы также можем сортировать по нескольким столбцам:
df.sort_values(by=["Churn", "Total day charge"], ascending=[True, False]).head()


# ### Индексирование и извлечение данных (.loc, .iloc)

# DataFrame может быть проиндексирован несколькими различными способами
# Чтобы получить один столбец, вы можете использовать DataFrame['Name'] конструкцию. 
# Давайте воспользуемся этим, чтобы ответить на вопрос только об этом столбце: какова доля ушедших пользователей в нашем фрейме данных?

# In[13]:


df["Churn"].mean()
#14,5% на самом деле очень плохо для компании; такой уровень оттока может привести компанию к банкротству.


# Булева индексация с одним столбцом тоже очень удобна. Синтаксис такой df[P(df['Name'])], где P некоторое логическое условие, которое проверяется для каждого элемента Name столбца. Результатом такой индексации является DataFrame состоящий только из строк, удовлетворяющих P условию для Name столбца.

# Воспользуемся им, чтобы ответить на вопрос:
# 
# Каковы средние значения числовых характеристик для ушедших пользователей?

# In[14]:


df[df["Churn"] == 1].mean()


# Сколько времени (в среднем) ушедшие пользователи проводят в телефоне в дневное время?

# In[15]:


df[df["Churn"] == 1]["Total day minutes"].mean()


# Какова максимальная продолжительность международных звонков среди лояльных пользователей ( ), у которых нет международного тарифного плана? Churn == 0

# In[16]:


df[(df["Churn"] == 0) & (df["International plan"] == "No")]["Total intl minutes"].max()


# DataFrames можно индексировать по имени столбца (метка) или имени строки (индекс) или по порядковому номеру строки. Метод loc используется для индексации по имени, а iloc() для индексации по номеру.

# В первом случае ниже мы говорим «дайте нам значения строк с индексом от 0 до 5 (включительно) и столбцов, от кода штата до кода города (включительно)».
# Во втором случае мы говорим «дайте нам значения первых пяти строк в первых трех столбцах» (как в типичном срезе Python: максимальное значение не включено).

# In[17]:


df.loc[0:5, "State":"Area code"]


# In[18]:


df.iloc[0:5, 0:3]


# Если нам нужна первая или последняя строка фрейма данных, мы можем использовать конструкцию df[:1] или df[-1:]:

# In[19]:


df[:1]


# ### Применение функций к ячейкам, столбцам и строкам (.apply, .map, .replace)

# Чтобы применить функции к каждому столбцу, используйте apply():

# In[20]:


df.apply(np.max)


# Этот apply метод также можно использовать для применения функции к каждой строке. Для этого укажите axis=1. Лямбда-функции очень удобны в таких сценариях. Например, если нам нужно выбрать все состояния, начинающиеся с «W», мы можем сделать это следующим образом:

# In[21]:


df[df["State"].apply(lambda state: state[0] == "W")].head()


# Этот map метод можно использовать для замены значений в столбце, передав в качестве аргумента словарь формы :{old_value: new_value}

# In[22]:


d = {"No": False, "Yes": True}
df["International plan"] = df["International plan"].map(d)
df.head()


# Почти то же самое можно сделать с помощью replace метода
# Разница в обработке значений, отсутствующих в словаре отображения
# Есть небольшая разница. Метод `replace` ничего не сделает со значениями, которых нет в словаре отображения, в то время как `map` изменит их на NaN).
# df = df.replace({"Voice mail plan": d})
# df.head()

# ### Группировка (.groupby, .agg)

# df.groupby(by=grouping_columns)[columns_to_show].function()

# 1.Во-первых, groupby метод делит grouping_columns на их значения. Они становятся новым индексом в результирующем кадре данных.
# 
# 2.Затем выбираются интересующие столбцы (columns_to_show). Если columns_to_show не включено, будут включены все предложения, не относящиеся к groupby.
# 
# 3.Наконец, к полученным группам применяются одна или несколько функций по выбранным столбцам.
# 
# Вот пример, где мы группируем данные по значениям Churn переменной и выводим статистику по трем столбцам в каждой группе:

# In[ ]:


columns_to_show = ["Total day minutes", "Total eve minutes", "Total night minutes"]

df.groupby(["Churn"])[columns_to_show].describe(percentiles=[])


# Сделаем то же самое, но немного иначе, передав список функций в agg():

# In[ ]:


columns_to_show = ["Total day minutes", "Total eve minutes", "Total night minutes"]

df.groupby(["Churn"])[columns_to_show].agg([np.mean, np.std, np.min, np.max])


# ### Сводные таблицы (.crosstab, .pivot_table )

# Предположим, мы хотим посмотреть, как распределяются наблюдения в нашем наборе данных в контексте двух переменных — Churn и International plan. Для этого мы можем построить таблицу непредвиденных обстоятельств, используя метод: crosstab

# In[ ]:


pd.crosstab(df["Churn"], df["International plan"])


# In[ ]:


pd.crosstab(df["Churn"], df["Voice mail plan"], normalize=True)


# Мы видим, что большинство пользователей лояльны и не пользуются дополнительными услугами (Международный тариф/Голосовая почта).

# Это будет напоминать сводные таблицы тем, кто знаком с Excel. И, конечно же, в Pandas реализованы сводные таблицы: 
# 
# pivot_table метод принимает следующие параметры:
# 
# values– список переменных для расчета статистики,
# 
# index– список переменных для группировки данных,
# 
# aggfunc– какую статистику нам нужно рассчитать для групп, напр. сумма, среднее значение, максимум, минимум или что-то еще.
# 
# Давайте посмотрим на среднее количество дневных, вечерних и ночных звонков по коду города:

# In[ ]:


df.pivot_table(
    ["Total day calls", "Total eve calls", "Total night calls"],
    ["Area code"],
    aggfunc="mean",
)


# ### Преобразования DataFrame (.insert, .drop)

# Как и многие другие вещи в Pandas, добавление столбцов в a DataFrame выполнимо разными способами.
# 
# Например, если мы хотим подсчитать общее количество звонков для всех пользователей, давайте создадим total_calls серию и вставим его в DataFrame:

# In[ ]:


total_calls = (
    df["Total day calls"]
    + df["Total eve calls"]
    + df["Total night calls"]
    + df["Total intl calls"]
)
df.insert(loc=len(df.columns), column="Total calls", value=total_calls)
# loc - это количество столбцов, после которых следует вставить объект серии
# мы устанавливаем для него значение len(df.columns), чтобы вставить его в самый конец фрейма данных
df.head()


# Можно проще добавить столбец, не создавая промежуточный экземпляр Series:

# In[ ]:


df["Total charge"] = (
    df["Total day charge"]
    + df["Total eve charge"]
    + df["Total night charge"]
    + df["Total intl charge"]
)
df.head()


# Чтобы удалить столбцы или строки, используйте drop метод, передавая необходимые индексы и axis параметр ( 1 если вы удаляете столбцы, и ничего или 0 если вы удаляете строки). Аргумент inplace сообщает, следует ли изменить исходный DataFrame. При inplace=False этот drop метод не изменяет существующий DataFrame и возвращает новый с удаленными строками или столбцами. С inplace=True, он изменяет DataFrame.

# In[ ]:


# избавимся от только что созданных столбцов
df.drop(["Total charge", "Total calls"], axis=1, inplace=True)
# и вот как вы можете удалить строки
df.drop([1, 2]).head()


# ## 2. Первая попытка предсказать отток клиентов 

# Давайте посмотрим, как скорость оттока связана с функцией международного плана. 
# Мы сделаем это с помощью crosstab таблицы непредвиденных обстоятельств, а также с помощью визуального анализа Seaborn(однако визуальный анализ будет более подробно рассмотрен в следующей статье).

# In[27]:


pd.crosstab(df["Churn"], df["International plan"], margins=True)
#margins=True показывает колонку с общим количеством


# In[25]:


#некоторые импортированные данные для настройки построения графика
import matplotlib.pyplot as plt

# !pip install seaborn
import seaborn as sns

# импортируйте несколько приятных настроек vis
sns.set()
# Графика в формате Retina более четкая и разборчивая
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")


# In[26]:


sns.countplot(x="International plan", hue="Churn", data=df);


# Мы видим, что с International Plan скорость оттока намного выше, что является интересным наблюдением!
# Возможно, большие и плохо контролируемые расходы на международные звонки очень конфликтны и приводят к недовольству клиентов оператора связи.
# 
# Далее рассмотрим еще одну важную функцию — звонки в службу поддержки. Также составим сводную таблицу и картинку.

# In[28]:


pd.crosstab(df["Churn"], df["Customer service calls"], margins=True)


# In[29]:


sns.countplot(x="Customer service calls", hue="Churn", data=df);


# Хотя это не так очевидно из сводной таблицы, на приведенном выше графике легко увидеть, что скорость оттока клиентов резко возрастает после 4 обращений в службу поддержки клиентов и выше.
# 
# Теперь давайте добавим бинарную функцию в наш DataFrame — Customer service calls > 3. И еще раз, давайте посмотрим, как это связано с оттоком.

# In[30]:


df["Many_service_calls"] = (df["Customer service calls"] > 3).astype("int")

pd.crosstab(df["Many_service_calls"], df["Churn"], margins=True)


# In[31]:


sns.countplot(x="Many_service_calls", hue="Churn", data=df);


# Давайте создадим еще одну таблицу непредвиденных обстоятельств, которая связывает Churn с International plan и только что созданным Many_service_calls.

# In[32]:


pd.crosstab(df["Many_service_calls"] & df["International plan"], df["Churn"])


# Следовательно, прогнозируя, что клиент не лоялен ( Churn =1) в случае, когда количество обращений в сервисный центр больше 3 и добавлен международный тариф (и прогнозируя Churn =0 в противном случае), можно ожидать точности 85,8% (ошибаемся всего 464+9 раз). Это число, 85,8%, которое мы получили с помощью этого очень простого рассуждения, служит хорошей отправной точкой ( базовой линией ) для дальнейших моделей машинного обучения, которые мы будем строить.
# 
# По мере прохождения этого курса вспомните, что до появления машинного обучения процесс анализа данных выглядел примерно так. Давайте подытожим то, что мы рассмотрели:
# 
# Доля лояльных клиентов в наборе данных составляет 85,5%. Самая наивная модель, которая всегда предсказывает «лояльного клиента» на таких данных, угадает примерно в 85,5% всех случаев. То есть доля правильных ответов ( точность ) последующих моделей должна быть не меньше этого числа и, надеюсь, будет значительно выше;
# 
# С помощью простого прогноза, который может быть выражен следующей формулой: International plan = True & Customer Service calls > 3 => Churn = 1, else Churn = 0, мы можем ожидать уровень угадывания 85,8%, что чуть выше 85,5%.
# Далее мы поговорим о деревьях решений и разберемся, как находить такие правила автоматически, основываясь только на входных данных;
# 
# Мы получили эти два базовых показателя без применения машинного обучения, и они послужат отправной точкой для наших последующих моделей. Если окажется, что с огромными усилиями мы повышаем точность, допустим, всего на 0,5%, то, возможно, мы что-то делаем не так, и достаточно ограничиться простой моделью «если-иначе» с двумя условиями;
# 
# Перед обучением сложных моделей рекомендуется немного повозиться с данными, построить несколько графиков и проверить простые предположения. Более того, в бизнес-приложениях машинного обучения обычно начинают с простых решений, а затем экспериментируют с более сложными.
